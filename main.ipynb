{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84309ab3-85fd-4a9b-ba2c-90778b3b9d64",
   "metadata": {},
   "source": [
    "# Machine Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917cc457-0659-4bc7-a25c-a1b125cb4ced",
   "metadata": {},
   "source": [
    "## 1. Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1896c1f-1ed4-4629-92ef-5a0d3d93b935",
   "metadata": {},
   "source": [
    "The initialisation modules import and parse the calls and weather data, saved in `init.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d578928-2b2b-461c-9c3e-f428a569bd6c",
   "metadata": {},
   "source": [
    "In `calls_parser`, the call catabase is parsed. In particular, the dates and times are converted to the standard form with time zone.\n",
    "\n",
    "There are events at the edge of standard and daylight times, and are ambiguous, since in the database the time zone is not indicated. I decide to drop these events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182914cb-e479-4ffb-96d6-1670782988d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile init.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import process_time as timer\n",
    "\n",
    "def calls_parser(fname='./data/calls.csv') :\n",
    "    print('Loading raw Seattle 911 calls database from ' + fname)\n",
    "    tim = timer()\n",
    "    calls_df = pd.read_csv(fname)\n",
    "    print('Raw Seattle 911 calls database loaded in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    calls_df['Datetime'] = pd.to_datetime(calls_df['Datetime'],\\\n",
    "                                          format=\"%m/%d/%Y %I:%M:%S %p\"\\\n",
    "                                         ).dt.tz_localize(tz='US/Pacific',\\\n",
    "                                                          ambiguous='NaT')\n",
    "\n",
    "    calls_df.set_index('Datetime', inplace=True)\n",
    "    calls_df.index.set_names('datetime', inplace=True)\n",
    "    calls_df.sort_index(inplace=True)\n",
    "    \n",
    "    calls_df.dropna(inplace=True) # see section 1 in main.ipynb\n",
    "\n",
    "    return calls_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54cd8c8-d1c2-4ddc-8625-953f5f819094",
   "metadata": {},
   "source": [
    "In `init_calls`, the parsed calls database is saved. I use `parquet` format due to [performance](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#performance-considerations) and compatibility considerations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f20b963-6de2-4b55-9577-b9aa09fe19ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a init.py\n",
    "\n",
    "def init_calls() :\n",
    "    calls_df_pt = './tmp/calls_df.parquet'\n",
    "    \n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "    if os.path.isfile(calls_df_pt) :\n",
    "        print('Loading parsed Seattle 911 calls database from ' + str(calls_df_pt))\n",
    "        tim = timer()\n",
    "        calls_df = pd.read_parquet(calls_df_pt) # fast and compat output\n",
    "        print('Parsed Seattle 911 calls database loaded in ' + str(timer() - tim) + ' s')\n",
    "    else :\n",
    "        calls_pt = './data/calls.csv'\n",
    "    \n",
    "        if not os.path.exists(calls_pt) :\n",
    "            print('Downloading missing raw Seattle 911 calls database to ' + calls_pt)\n",
    "            os.system('cat get_calls.sh | sh')\n",
    "    \n",
    "        calls_df = calls_parser(calls_pt)\n",
    "\n",
    "        print('Saving parsed Seattle 911 calls database to ' + calls_df_pt)\n",
    "        tim = timer()\n",
    "        calls_df.to_parquet(calls_df_pt) # fast and compat input\n",
    "        print('Parsed Seattle 911 calls database saved in ' + str(timer() - tim) + ' s')\n",
    "    \n",
    "    return calls_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967a4a6-d777-445b-91d7-e5caeb91911a",
   "metadata": {},
   "source": [
    "In `weather_parser` and `init_weather`, the weather data is parsed and saved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b81051-d373-439d-872e-b7c89324d998",
   "metadata": {},
   "source": [
    "There are also duplicates, i.e. more than one items within the same hour. I decide to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "151cf118-95e2-4862-a36b-d13090946fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a init.py\n",
    "\n",
    "def weather_parser(fname='./data/Seattle Weatherdata 2002 to 2020.csv') :\n",
    "    print('Loading raw Seattle weather database from ' + fname)\n",
    "    tim = timer()\n",
    "    wtr_df = pd.read_csv(fname)\n",
    "    print('Raw Seattle weather database loaded in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    wtr_df['datetime'] = pd.to_datetime(wtr_df['dt'], unit='s').\\\n",
    "                            dt.tz_localize(tz='UTC').\\\n",
    "                            dt.tz_convert('US/Pacific')\n",
    "    wtr_df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    return(wtr_df)\n",
    "\n",
    "\n",
    "def init_weather() :\n",
    "    \n",
    "    wtr_df_pt = './tmp/wtr_df.parquet'\n",
    "    \n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "    if os.path.isfile(wtr_df_pt) :\n",
    "        print('Loading parsed Seattle weather database from ' + str(wtr_df_pt))\n",
    "        tim = timer()\n",
    "        wtr_df = pd.read_parquet(wtr_df_pt)\n",
    "        print('Parsed Seattle weather database loaded in ' + str(timer() - tim) + ' s')\n",
    "    else :\n",
    "        wtr_df = weather_parser(fname='./data/Seattle Weatherdata 2002 to 2020.csv')\n",
    "\n",
    "        print('Saving parsed Seattle weather database to ' + wtr_df_pt)\n",
    "        tim = timer()\n",
    "        wtr_df.to_parquet(wtr_df_pt)\n",
    "        print('Parsed Seattle weather database saved in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    wtr_df = wtr_df[~wtr_df.index.duplicated()] # see main.ipynb\n",
    "\n",
    "    return(wtr_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d167de1c-efda-41e5-86a5-9ffb16926d1c",
   "metadata": {},
   "source": [
    "In `feature_parser`, I choose a subset of all weather data as features. The criteria are unfortunately somewhat subjective. Time, day, calendar week and year are also included, because I expect systematic influences by these factors.\n",
    "\n",
    "In `y_parser`, I cound event numbers from the parsed database.\n",
    "\n",
    "In `init`, I combine the feature and target databases and save it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74abc5ac-27cf-4bce-a9d5-5fa5910afe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a init.py\n",
    "\n",
    "def feature_parser(wtr_df) :\n",
    "    x = wtr_df[['temp', #'temp_min', 'temp_max',\n",
    "                'pressure', 'humidity', 'wind_speed', 'wind_deg', 'weather_id']]\n",
    "    x.index.set_names('datetime', inplace=True)\n",
    "\n",
    "    x_tim = x.index.isocalendar()\n",
    "    x_tim['hour'] = x.index.hour\n",
    "    x = x_tim.join(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def y_parser(calls_df) :\n",
    "    y = calls_df['Incident Number'].resample('H').count().to_frame('incident_count')\n",
    "    y.index.set_names('datetime', inplace=True)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def init() :\n",
    "    \n",
    "    xy_df_pt = './tmp/xy_df.parquet'\n",
    "    \n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "    if os.path.isfile(xy_df_pt) :\n",
    "        print('Loading ML database from ' + str(xy_df_pt))\n",
    "        tim = timer()\n",
    "        xy_df = pd.read_parquet(xy_df_pt)\n",
    "        print('ML database loaded in ' + str(timer() - tim) + ' s')\n",
    "    else :\n",
    "        \n",
    "        tim = timer()\n",
    "        calls_df = init_calls()\n",
    "        wtr_df = init_weather()\n",
    "        \n",
    "        x_raw = feature_parser(wtr_df)\n",
    "\n",
    "        y_raw = y_parser(calls_df)\n",
    "        xy_df = y_raw.join(x_raw).dropna()\n",
    "\n",
    "        xy_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        print('Saving ML database to ' + xy_df_pt)\n",
    "        tim = timer()\n",
    "        xy_df.to_parquet(xy_df_pt) #, key='x', mode='w'\n",
    "        print('ML database saved in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    return xy_df.iloc[:, 1:], xy_df.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216523b-dba7-4c21-a515-f98386af2f38",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4249ecab-d3d4-4db6-a246-c7d638685ed0",
   "metadata": {},
   "source": [
    "Here I define the splitters and the preprocessing pipeline in `prep.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ad4a3-6e3a-40d2-b49d-eeef88ccfd30",
   "metadata": {},
   "source": [
    "In `fwd_splitter`, I use `TimeSeriesSplit` to get a series of back-testing training-testing pairs. Unfortunately this will not be used, since the best fit from e.g. 2003 - 2008 will not be quite useful for predicting events in 2021."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "60db8efe-7edf-46db-816f-d8456dbbe206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prep.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.preprocessing as skl_prep\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def fwd_splitter(n_spi=2, tr_week=261, te_week=52) :\n",
    "    return TimeSeriesSplit(n_splits=n_spi, max_train_size=tr_week*7*24, test_size=te_week*7*24)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e2016e-c4a6-4e7e-b54c-b0bf9efa3410",
   "metadata": {},
   "source": [
    "Here in `nai_splitter` I just naively split the last six years into five years (training) and one year (testing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2f22e60-57ef-4571-821e-45d9779fe59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a prep.py\n",
    "\n",
    "def nai_splitter() :\n",
    "    e1_ind = -365*24\n",
    "    tr_ind = (-365*5-1)*24+e1_ind\n",
    "    e2_ind = (-365*5-1)*24+2*e1_ind\n",
    "    return np.arange(tr_ind, e1_ind), np.arange(e1_ind, 0), np.arange(e2_ind, tr_ind)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cef180b-f020-40b7-8be3-7da56165d510",
   "metadata": {},
   "source": [
    "In `prep_ppl` I only use the `StandardScaler`, since I do not know what else to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1283c24-ac45-45ce-b70f-0a202d08253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a prep.py\n",
    "\n",
    "def prep_ppl() :\n",
    "    return(Pipeline([\n",
    "        ('std_scl', skl_prep.StandardScaler())\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b07ea8-e2fa-4741-a657-410f7697ecb2",
   "metadata": {},
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d28987-de43-406a-aab2-5c30b7de8528",
   "metadata": {},
   "source": [
    "Here I define the estimator in `model.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6592fc46-6ec7-4948-88d6-c5ec71496bb7",
   "metadata": {},
   "source": [
    "Since I do not know too much about machine learning, I just follow the suggestion and choose `GradientBoostingRegressor`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5cf4c1-4f7b-4e8c-a60d-ff7c83b4d197",
   "metadata": {},
   "source": [
    "Following [a suggestion](https://towardsdatascience.com/how-to-generate-prediction-intervals-with-scikit-learn-and-python-ab3899f992ed), I use Huber loss function with different `alpha` to make a prediction interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99a951d5-b3ad-4df4-a752-697d93c3031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "#from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "import prep\n",
    "\n",
    "def model_ppl(rseed=0) :\n",
    "    est_l = Pipeline([\n",
    "            ('preprocessor', prep.prep_ppl()),\n",
    "            ('regressor', GradientBoostingRegressor(\n",
    "                loss='huber',\n",
    "                alpha=.1,\n",
    "                random_state=rseed))])\n",
    "    est_m = Pipeline([\n",
    "            ('preprocessor', prep.prep_ppl()),\n",
    "            ('regressor', GradientBoostingRegressor(\n",
    "                loss='ls',\n",
    "                random_state=rseed))])\n",
    "    est_u = Pipeline([\n",
    "            ('preprocessor', prep.prep_ppl()),\n",
    "            ('regressor', GradientBoostingRegressor(\n",
    "                loss='huber',\n",
    "                alpha=.9,\n",
    "                random_state=rseed))])\n",
    "\n",
    "    return est_l, est_m, est_u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622d6cd-922b-4b42-913e-a901de79d446",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e280f66-3969-456e-8ebb-b664c184ea1a",
   "metadata": {},
   "source": [
    "The training pipeline is contained in `train.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c8ee1e-510c-4add-8bb3-752146aaa937",
   "metadata": {},
   "source": [
    "In `train`, I use `GridSearchCV` to decide a better set of parameters. However I do not understand much about them, so I only play with the `n_estimators`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185b8ac-070d-4cfa-beb2-29f02aafaf76",
   "metadata": {},
   "source": [
    "By the way, the Chinese data scientists also use 'alchemy practice' to indicate training. But here I do not use this term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "675d90c8-fc73-41fc-8bd6-1ba0d08ea6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import process_time as timer\n",
    "import joblib\n",
    "\n",
    "import init, prep, model\n",
    "\n",
    "def train_cv(est, p_grid, x, y, fname) :\n",
    "    print('Cross validating by Grid Search')\n",
    "    reg = GridSearchCV(estimator = est, param_grid=p_grid, n_jobs=-1, verbose=2)\n",
    "    reg.fit(x, y)\n",
    "    print(reg.best_params_)\n",
    "    joblib.dump(reg, fname)\n",
    "    print('Regressor dumped to' + fname)\n",
    "    \n",
    "    return reg\n",
    "\n",
    "def train(redo = False) :\n",
    "\n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "\n",
    "    train_l_pt, train_m_pt, train_u_pt = './tmp/reg_l.pkl', './tmp/reg_m.pkl', './tmp/reg_u.pkl'\n",
    "    if not redo and os.path.isfile(train_l_pt) :\n",
    "        reg_l = joblib.load(train_l_pt)\n",
    "        reg_m = joblib.load(train_m_pt)\n",
    "        reg_u = joblib.load(train_u_pt)\n",
    "        print('Regressor loaded from ' + train_l_pt + ', ' + train_m_pt + ', '+ train_u_pt)\n",
    "    else:\n",
    "        x, y = init.init()\n",
    "        tr_ind, e1_ind, e2_ind = prep.nai_splitter()\n",
    "    \n",
    "        p_grid = {\n",
    "            # 'regressor__learning_rate': [.1,], # .1\n",
    "            'regressor__n_estimators': [x + 100 for x in range(5)], # 100\n",
    "            # 'regressor__subsample': [1.], # 1.\n",
    "            # 'regressor__max_depth': [3], # 3\n",
    "            # 'regressor__max_features': ['auto'],\n",
    "            # 'regressor__tol': [1e-4],\n",
    "        }\n",
    "\n",
    "        tim = timer()\n",
    "        \n",
    "        est_l, est_m, est_u = model.model_ppl()\n",
    "        \n",
    "        reg_l = train_cv(est_l, p_grid, x.iloc[tr_ind], y.iloc[tr_ind], train_l_pt)\n",
    "        reg_m = train_cv(est_m, p_grid, x.iloc[tr_ind], y.iloc[tr_ind], train_m_pt)\n",
    "        reg_u = train_cv(est_u, p_grid, x.iloc[tr_ind], y.iloc[tr_ind], train_u_pt)\n",
    "\n",
    "    return reg_l, reg_m, reg_u\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ac26a-c241-4f48-9052-c105688ef14c",
   "metadata": {},
   "source": [
    "## 5. Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fad088d-2c79-4630-8504-b43c120b09af",
   "metadata": {},
   "source": [
    "In `main.py`, there is only the `main` function. If no input is given, it runs a demo by using a data generator `tester.data_gen`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44a491f6-13c2-479c-bf09-6f5a414b3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import sys, getopt, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import init, train, tester\n",
    "\n",
    "def usage() :\n",
    "    print('usage: python ' + sys.argv[0] + ' -w weather.csv [-c calls.csv] {--week, --month}')\n",
    "\n",
    "def main() :\n",
    "    try :\n",
    "        opts, args = getopt.getopt(sys.argv[1:], 'hw:c:WM', ['help', 'weather=', 'calls=', 'week', 'month'])\n",
    "    except getopt.GetoptError as err :\n",
    "        print(err)\n",
    "        usage()\n",
    "        sys.exit(2)\n",
    "\n",
    "    fwtr = ''\n",
    "    fcal = ''\n",
    "\n",
    "    if not opts:\n",
    "        usage()\n",
    "        sys.exit()\n",
    "\n",
    "    for o, a in opts :\n",
    "        if o in ('-h', '--help') :\n",
    "            usage()\n",
    "            sys.exit()\n",
    "        elif o in ('-w', '--weather') :\n",
    "            fwtr = a\n",
    "        elif o in ('-c', '--calls') :\n",
    "            fcal = a\n",
    "        elif o in ('-W', '--week'):\n",
    "            fwtr = './tmp/test_wtr.csv'\n",
    "            fcal = './tmp/test_cal.csv'\n",
    "            print('Work in back-testing mode (week) with ' + fwtr + ' and ' + fcal)\n",
    "            tester.data_gen(fwtr, fcal, dend='2020-01-07')\n",
    "        elif o in ('-M', '--month'):\n",
    "            fwtr = './tmp/test_wtr.csv'\n",
    "            fcal = './tmp/test_cal.csv'\n",
    "            print('Work in back-testing mode (month) with ' + fwtr + ' and ' + fcal)\n",
    "            tester.data_gen(fwtr, fcal, dend='2020-01-31')\n",
    "\n",
    "    wtr_new = init.weather_parser(fwtr)\n",
    "    wtr_new = wtr_new[~wtr_new.index.duplicated()]\n",
    "    x_new = init.feature_parser(wtr_new)\n",
    "\n",
    "    reg_l, reg_m, reg_u = train.train()\n",
    "\n",
    "    if fcal : # real data given\n",
    "        calls_new = init.calls_parser(fcal)\n",
    "        y_new = init.y_parser(calls_new)\n",
    "        tester.printer(x_new, y_new['incident_count'], reg_l, reg_m, reg_u)\n",
    "    else : # no real data\n",
    "        y_pel, y_pem, y_peu = reg_l.predict(x_new), reg_m.predict(x_new), reg_u.predict(x_new)\n",
    "        print('Middle prediction: ' + str(int(y_pem.sum())))\n",
    "        print('Prediction interval: [' + str(int(y_pel.sum())) + ', ' + str(int(y_peu.sum())) + ']')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
