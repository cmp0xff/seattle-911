{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84309ab3-85fd-4a9b-ba2c-90778b3b9d64",
   "metadata": {},
   "source": [
    "# Machine Learning Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917cc457-0659-4bc7-a25c-a1b125cb4ced",
   "metadata": {},
   "source": [
    "## 1. Initialisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1896c1f-1ed4-4629-92ef-5a0d3d93b935",
   "metadata": {},
   "source": [
    "Import calls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9209632-9d79-47a1-8787-f542325826da",
   "metadata": {},
   "source": [
    "[Performance](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html#performance-considerations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "182914cb-e479-4ffb-96d6-1670782988d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile init.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from time import process_time as timer\n",
    "\n",
    "def calls_parser(fname) :\n",
    "    print('Loading raw Seattle 911 calls database from ' + fname)\n",
    "    tim = timer()\n",
    "    calls_df = pd.read_csv(fname)\n",
    "    print('Raw Seattle 911 calls database loaded in ' + str(timer() - tim))\n",
    "\n",
    "    calls_df['Datetime'] = pd.to_datetime(calls_df['Datetime'],\\\n",
    "                                          format=\"%m/%d/%Y %I:%M:%S %p\"\\\n",
    "                                         ).dt.tz_localize(tz='US/Pacific',\\\n",
    "                                                          ambiguous='NaT')\n",
    "\n",
    "    calls_df.dropna(inplace=True)\n",
    "    calls_df.set_index('Datetime', inplace=True)\n",
    "    calls_df.index.set_names('datetime', inplace=True)\n",
    "    calls_df.sort_index(inplace=True)\n",
    "\n",
    "    return calls_df\n",
    "\n",
    "\n",
    "def init_calls() :\n",
    "    # Convert to pandas DataFrame\n",
    "    \n",
    "    calls_df_pt = './tmp/calls_df.parquet'\n",
    "    \n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "    if os.path.isfile(calls_df_pt) :\n",
    "        print('Loading parsed Seattle 911 calls database from ' + str(calls_df_pt))\n",
    "        tim = timer()\n",
    "        calls_df = pd.read_parquet(calls_df_pt) # pd.read_hdf(calls_df_pt, key='c', mode='r')\n",
    "        print('Parsed Seattle 911 calls database loaded in ' + str(timer() - tim) + ' s')\n",
    "    else :\n",
    "        calls_pt = './data/calls.csv'\n",
    "    \n",
    "        if not os.path.exists(calls_pt) :\n",
    "            print('Downloading missing raw Seattle 911 calls database to ' + calls_pt)\n",
    "            os.system('cat get_calls.sh | sh')\n",
    "    \n",
    "        calls_df = calls_parser(calls_pt)\n",
    "\n",
    "        print('Saving parsed Seattle 911 calls database to ' + calls_df_pt)\n",
    "        tim = timer()\n",
    "        calls_df.to_parquet(calls_df_pt) # calls_df.to_hdf(calls_df_pt, key='c', mode='w')\n",
    "        print('Parsed Seattle 911 calls database saved in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "        # results_df['type'] = results_df['type'].astype('category')\n",
    "    \n",
    "    return calls_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e967a4a6-d777-445b-91d7-e5caeb91911a",
   "metadata": {},
   "source": [
    "Import weather data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "151cf118-95e2-4862-a36b-d13090946fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a init.py\n",
    "\n",
    "def weather_parser(fname) :\n",
    "    print('Loading raw Seattle weather database from ' + fname)\n",
    "    tim = timer()\n",
    "    wtr_df = pd.read_csv(fname)\n",
    "    print('Raw Seattle weather database loaded in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    wtr_df['datetime'] = pd.to_datetime(wtr_df['dt'], unit='s').\\\n",
    "                            dt.tz_localize(tz='UTC').\\\n",
    "                            dt.tz_convert('US/Pacific')\n",
    "    wtr_df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    return(wtr_df)\n",
    "\n",
    "\n",
    "def init_weather() :\n",
    "    \n",
    "    wtr_df_pt = './tmp/wtr_df.parquet'\n",
    "    \n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "    if os.path.isfile(wtr_df_pt) :\n",
    "        print('Loading parsed Seattle weather database from ' + str(wtr_df_pt))\n",
    "        tim = timer()\n",
    "        wtr_df = pd.read_parquet(wtr_df_pt) # wtr_df = pd.read_hdf(wtr_df_pt, key='w', mode='r')\n",
    "        # wtr_df.index.to_datetime().dt.tz_convert('US/Pacific')\n",
    "        print('Parsed Seattle weather database loaded in ' + str(timer() - tim) + ' s')\n",
    "    else :\n",
    "        wtr_df = weather_parser('./data/Seattle Weatherdata 2002 to 2020.csv')\n",
    "\n",
    "        wtr_df = wtr_df[~wtr_df.index.duplicated()]\n",
    "\n",
    "        print('Saving parsed Seattle weather database to ' + wtr_df_pt)\n",
    "        tim = timer()\n",
    "        wtr_df.to_parquet(wtr_df_pt) # wtr_df.to_hdf(wtr_df_pt, key='w', mode='w')\n",
    "        print('Parsed Seattle weather database saved in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    return(wtr_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74abc5ac-27cf-4bce-a9d5-5fa5910afe93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to init.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a init.py\n",
    "\n",
    "def feature_parser(wtr_df) :\n",
    "    x = wtr_df[['temp', 'temp_min', 'temp_max', 'pressure', 'humidity', 'wind_speed', 'wind_deg', 'weather_id']]\n",
    "    x.index.set_names('datetime', inplace=True)\n",
    "\n",
    "    x_tim = x.index.isocalendar()\n",
    "    x_tim['hour'] = x.index.hour\n",
    "    x = x_tim.join(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def y_parser(calls_df) :\n",
    "    y = calls_df['Incident Number'].resample('H').count().to_frame('incident_count')\n",
    "    y.index.set_names('datetime', inplace=True)\n",
    "    \n",
    "    return y\n",
    "\n",
    "\n",
    "def init() :\n",
    "    \n",
    "    xy_df_pt = './tmp/xy_df.parquet'\n",
    "    \n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "    if os.path.isfile(xy_df_pt) :\n",
    "        print('Loading ML database from ' + str(xy_df_pt))\n",
    "        tim = timer()\n",
    "        xy_df = pd.read_parquet(xy_df_pt)\n",
    "        print('ML database loaded in ' + str(timer() - tim) + ' s')\n",
    "    else :\n",
    "        \n",
    "        tim = timer()\n",
    "        calls_df = init_calls()\n",
    "        wtr_df = init_weather()\n",
    "        \n",
    "        x_raw = feature_parser(wtr_df)\n",
    "\n",
    "        # x_raw.drop_duplicates(inplace=True)\n",
    "\n",
    "        y_raw = y_parser(calls_df)\n",
    "        xy_df = y_raw.join(x_raw).dropna()\n",
    "        # xy_df.index.set_names('datetime', inplace=True)\n",
    "        # xy_df['hour'] = xy_df.index.hour\n",
    "\n",
    "        xy_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        print('Saving ML database to ' + xy_df_pt)\n",
    "        tim = timer()\n",
    "        xy_df.to_parquet(xy_df_pt) #, key='x', mode='w'\n",
    "        print('ML database saved in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "    return xy_df.iloc[:, 1:], xy_df.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2216523b-dba7-4c21-a515-f98386af2f38",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60db8efe-7edf-46db-816f-d8456dbbe206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile prep.py\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.preprocessing as skl_prep\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def fwd_splitter(n_spi=2, tr_week=261, te_week=52) :\n",
    "    return TimeSeriesSplit(n_splits=n_spi, max_train_size=tr_week*7*24, test_size=te_week*7*24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2f22e60-57ef-4571-821e-45d9779fe59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a prep.py\n",
    "\n",
    "def nai_splitter() :\n",
    "    e1_ind = -365*24\n",
    "    tr_ind = (-365*5-1)*24+e1_ind\n",
    "    e2_ind = (-365*5-1)*24+2*e1_ind\n",
    "    return np.arange(tr_ind, e1_ind), np.arange(e1_ind, 0), np.arange(e2_ind, tr_ind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a1283c24-ac45-45ce-b70f-0a202d08253a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a prep.py\n",
    "\n",
    "def prep_ppl() :\n",
    "    return(Pipeline([\n",
    "        ('std_scl', skl_prep.StandardScaler())\n",
    "    ]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b07ea8-e2fa-4741-a657-410f7697ecb2",
   "metadata": {},
   "source": [
    "## 3. Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "99a951d5-b3ad-4df4-a752-697d93c3031f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting model.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile model.py\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "import prep\n",
    "\n",
    "def model_ppl(rseed=0) :\n",
    "    return Pipeline([\n",
    "        ('preprocessor', prep.prep_ppl()),\n",
    "        ('regressor', GradientBoostingRegressor(random_state=rseed))\n",
    "    ])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5622d6cd-922b-4b42-913e-a901de79d446",
   "metadata": {},
   "source": [
    "## 4. Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3185b8ac-070d-4cfa-beb2-29f02aafaf76",
   "metadata": {},
   "source": [
    "[parameters](https://stackoverflow.com/a/49501713/2682621)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "675d90c8-fc73-41fc-8bd6-1ba0d08ea6d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train.py\n",
    "\n",
    "import os\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from time import process_time as timer\n",
    "import joblib\n",
    "\n",
    "import init\n",
    "import prep\n",
    "import model\n",
    "\n",
    "def train() :\n",
    "\n",
    "    if not os.path.exists('./tmp') :\n",
    "        os.system('mkdir tmp')\n",
    "\n",
    "    train_pt = './tmp/reg.pkl'\n",
    "    if os.path.isfile(train_pt) :\n",
    "        reg = joblib.load(train_pt)\n",
    "        print('Regressor loaded from ' + train_pt)\n",
    "    else:\n",
    "        x, y = init.init()\n",
    "        tr_ind, e1_ind, e2_ind = prep.nai_splitter()\n",
    "    \n",
    "        p_grid = {\n",
    "            'regressor__learning_rate': [.5, .25, .1, .05, .01],\n",
    "            'regressor__n_estimators': [25, 50, 100, 200, 400],\n",
    "            'regressor__max_depth': [3, 4, 5]\n",
    "        }\n",
    "\n",
    "        tim = timer()\n",
    "        reg = GridSearchCV(estimator=model.model_ppl(),\n",
    "                           param_grid=p_grid,\n",
    "                           n_jobs=-1,\n",
    "                           verbose=2)\n",
    "\n",
    "        print('Cross validating by Grid Search')\n",
    "        reg.fit(x.iloc[tr_ind], y.iloc[tr_ind])\n",
    "        # print('Cross validating by Grid Search successful in ' + str(timer() - tim) + ' s')\n",
    "\n",
    "        joblib.dump(reg, train_pt)\n",
    "        print('Regressor dumped to ' + train_pt)\n",
    "        \n",
    "    return reg\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688ac26a-c241-4f48-9052-c105688ef14c",
   "metadata": {},
   "source": [
    "## 5. Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44a491f6-13c2-479c-bf09-6f5a414b3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing main.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import sys\n",
    "\n",
    "import init\n",
    "import train\n",
    "\n",
    "def main() :\n",
    "    if len(sys.argv) == 1 or len(sys.argv) > 3 :\n",
    "        print('usage: python main.py input.csv [calls.csv]')\n",
    "    else :\n",
    "        wtr_new = init.weather_parser(sys.argv[1])\n",
    "        x_new = init.feature_parser(wtr_new)\n",
    "        reg = train.train()\n",
    "        y_pew = reg.predict(x_new)\n",
    "        \n",
    "        y_pew.to_csv('output.csv')\n",
    "        \n",
    "        if len(sys.argv) == 3 :\n",
    "            calls_new = init.calls_parser()\n",
    "            print('Score: ' + str(reg.score(x_new, y_new)))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
